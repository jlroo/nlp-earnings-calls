{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earning Calls Transcripts Text Extractions<br>\n",
    "\n",
    "JOSE LUIS RODRIGUEZ <br> Student ID: 504732\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the text normalization function from Text_Normalization_Function.ipynb we used in Lab 2 (Important: Make sure Text_Normalization_Function.ipynb file is in the same directory as the current notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jlroo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jlroo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jlroo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jlroo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%run ./normalization.ipynb #defining text normalization function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Exploring Data\n",
    "\n",
    "Download the dataset from the dataset sklearn's collection of datasets (sklearn.datasets). The dataset we need is called fetch_20newsgroups:<br>\n",
    "\n",
    "You can check out the documentation for the dataset here: https://bit.ly/3aM5tUo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 20 newsgroups (topics) available, we will use posts on **4** topics: <br><br>-atheism<br> -religion<br> -computer graphics<br>-science<br><br> You can refer to those newsgroups as **classes** or **categories**. Note right away that the \"atheism\" and \"religion\" classes are likely **similar** to each other: people might be using similar words when they talk about atheism and religion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create our **training** and **testing** datasets. We do that by picking the posts belonging to the selected classes, marked in the dataset \"test\" or \"train\". We remove headers (likely, email or letter headers), footers (containing author's signatures, etc.), and quotes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = fetch_20newsgroups(categories = categories,\n",
    "                                  subset = 'train', \n",
    "                                  remove = ('headers', 'footers', 'quotes')) \n",
    "\n",
    "corpus_test = fetch_20newsgroups(categories = categories,\n",
    "                                 subset='test', \n",
    "                                 remove=('headers', 'footers', 'quotes')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the data by looking at the training text corpus. First, have a look at one of the posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acorn Replay running on a 25MHz ARM 3 processor (the ARM 3 is about 20% slower\n",
      "than the ARM 6) does this in software (off a standard CD-ROM). 16 bit colour at\n",
      "about the same resolution (so what if the computer only has 8 bit colour\n",
      "support, real-time dithering too...). The 3D0/O is supposed to have a couple of\n",
      "DSPs - the ARM being used for housekeeping.\n",
      "\n",
      "\n",
      "A 25MHz ARM 6xx should clock around 20 ARM MIPS, say 18 flat out. Depends\n",
      "really on the surrounding system and whether you are talking ARM6x or ARM6xx\n",
      "(the latter has a cache, and so is essential to run at this kind of speed with\n",
      "slower memory).\n",
      "\n",
      "I'll stop saying things there 'cos I'll hopefully be working for ARM after\n",
      "graduation...\n",
      "\n",
      "Mike\n",
      "\n",
      "PS Don't pay heed to what reps from Philips say; if the 3D0/O doesn't beat the\n",
      "   pants off 3DI then I'll eat this postscript.\n"
     ]
    }
   ],
   "source": [
    "print(corpus_train.data[7])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class labels for each message that you will be using for training and testing are encoded as numbers and can be accessed via attribute **.target** and their names can be accessed via attribute **.target_names**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category names:  ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "Categories for first 10 observations:  [1 3 2 0 2 0 2 1 2 1]\n",
      "Number of posts in the training dataset:  2034\n"
     ]
    }
   ],
   "source": [
    "print(\"Category names: \", corpus_train.target_names)    \n",
    "print(\"Categories for first 10 observations: \", corpus_train.target[:10])     \n",
    "print(\"Number of posts in the training dataset: \", corpus_train.filenames.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a function and call it **fmat_descr_fun** to be used later to describe the feature matrix (vectorized corpus). The function prints out the dimensions of the matrix, share of non-zero elements in the matrix and so on. The function takes two inputs: the feature matrix and your vectorizer function. <br><br>In what follows later, you will see the descriptives that the function produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmat_descr_fun(your_feature_matrix, your_vectorizer):\n",
    "    print(\"Dimensions (number of posts, number of features): \", your_feature_matrix.shape)  \n",
    "    print(\"The first 5 features - names: \", your_vectorizer.get_feature_names()[0:5]) \n",
    "    print(\"Share of non-zero elements in the matrix: \", \n",
    "          your_feature_matrix.nnz / (float(your_feature_matrix.shape[0]) * float(your_feature_matrix.shape[1])))\n",
    "    print(\"Average number of features present, per post: \", \n",
    "          round(your_feature_matrix.nnz/float(your_feature_matrix.shape[0]),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for TRAINING Data\n",
    "\n",
    "Let's do feature extraction for our **TRAINING** data using the **\"Bag-of-words\"** method and **TF-IDF** method (optional).\n",
    "\n",
    "#### \"Bag-of-words\" Vectorization for TRAINING data\n",
    "\n",
    "As you remember from Lab 2, to do the Bag-of-Words vectorization, we can use the CountVectorizer function from the sklearn package: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our Bag-of-Words vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the vectorizer we defined, bow_vectorizer, to our training dataset, **without normalizing** it first (though tokenization will be done by the vectorizer). Remember, to create the corpus vocabulary and to vectorize the data accroding to that vocabulary, we use the **.fit_transform** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_bow = bow_vectorizer.fit_transform(corpus_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the Bag-of-Words matrix that you got by applying the function **fmat_descr_fun** we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (number of posts, number of features):  (2034, 26879)\n",
      "The first 5 features - names:  ['00', '000', '0000', '00000', '000000']\n",
      "Share of non-zero elements in the matrix:  0.0035978272269590263\n",
      "Average number of features present, per post:  96.7\n"
     ]
    }
   ],
   "source": [
    "fmat_descr_fun(corpus_train_bow, bow_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first 5 rows in the resulting matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>000000</th>\n",
       "      <th>000005102000</th>\n",
       "      <th>000062david42</th>\n",
       "      <th>0001</th>\n",
       "      <th>000100255pixel</th>\n",
       "      <th>00041032</th>\n",
       "      <th>...</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurvanism</th>\n",
       "      <th>zus</th>\n",
       "      <th>zvi</th>\n",
       "      <th>zwaartepunten</th>\n",
       "      <th>zwak</th>\n",
       "      <th>zwakke</th>\n",
       "      <th>zware</th>\n",
       "      <th>zwarte</th>\n",
       "      <th>zyxel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26879 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0000  00000  000000  000005102000  000062david42  0001  \\\n",
       "0   0    0     0      0       0             0              0     0   \n",
       "1   0    0     0      0       0             0              0     0   \n",
       "2   0    0     0      0       0             0              0     0   \n",
       "3   0    0     0      0       0             0              0     0   \n",
       "4   0    0     0      0       0             0              0     0   \n",
       "\n",
       "   000100255pixel  00041032  ...  zurich  zurvanism  zus  zvi  zwaartepunten  \\\n",
       "0               0         0  ...       0          0    0    0              0   \n",
       "1               0         0  ...       0          0    0    0              0   \n",
       "2               0         0  ...       0          0    0    0              0   \n",
       "3               0         0  ...       0          0    0    0              0   \n",
       "4               0         0  ...       0          0    0    0              0   \n",
       "\n",
       "   zwak  zwakke  zware  zwarte  zyxel  \n",
       "0     0       0      0       0      0  \n",
       "1     0       0      0       0      0  \n",
       "2     0       0      0       0      0  \n",
       "3     0       0      0       0      0  \n",
       "4     0       0      0       0      0  \n",
       "\n",
       "[5 rows x 26879 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert vectorized data to a dataframe and give columns their names\n",
    "corpus_train_bow_table = pd.DataFrame(data = corpus_train_bow.todense(),\n",
    "                                      columns = bow_vectorizer.get_feature_names())\n",
    "corpus_train_bow_table.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green>EXERCISE 1:</font>**\n",
    "**<font color=green> Answer the following questions using the descriptives of the Bag-of-Words matrix:</font>** <br><br>\n",
    "**<font color=green>1.1. How many features does the Bag-of-Words matrix contain?</font>** <br><br> \n",
    "\n",
    "**ANSWER:** 26,879"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>1.2. Is the Bag-of-Words matrix sparse? Explain your answer: </font>** <br><br> \n",
    "\n",
    "**ANSWER:** Yes, it is sparse there are only 0.0035% of non-zero elements in the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize our training data and call the normalized corpus **NORM_corpus_train**. Note that it will take some time for the normalization function to finish its job as you are working with large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_corpus_train = normalize_corpus(corpus_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green>EXERCISE 2:</font>**\n",
    "**<font color=green>First, vectorize the normalized training corpus (NORM_corpus_train) using the Bag-of-Words approach and name the vectorized corpus NORM_corpus_train_bow.** <br><br>\n",
    "Your code:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_corpus_train_bow = bow_vectorizer.fit_transform(NORM_corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>\n",
    "Second, describe the normalized training corpus using the fmat_descr_fun function.** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (number of posts, number of features):  (2034, 21061)\n",
      "The first 5 features - names:  ['000062david42', '000100255pixel', '000usd', '001200201pixel', '00index']\n",
      "Share of non-zero elements in the matrix:  0.002958676433492318\n",
      "Average number of features present, per post:  62.3\n"
     ]
    }
   ],
   "source": [
    "fmat_descr_fun(NORM_corpus_train_bow, bow_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>2.1. What differences do you see between normalized and non-normalized feature matrices?</font>** <br><br>\n",
    "\n",
    "**ANSWER:** Here are the main differences:\n",
    "1. Number of features decreased from 26,879 to 21,961 (normalized)\n",
    "2. First features in normalized matrix started giving us some insights\n",
    "3. Normalized matrix is even more sparse\n",
    "4. number of features per post decreased by almost half from 96.7 to 63.3 (normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection for TRAINING Data\n",
    "Let's select the best features for your normalized training corpus, **NORM_corpus_train_bow**, using the **chi-squared statistic** and **mutial information (MI)**, whcih is based on the ideas of **entropy**.<br><br>**IMPORTANT!** You need to be done with the previous EXERCISE to be able to continue. <br><br>We need to import the feature selection function **SelectKBest** first. This function selects k best features based on the results of a test (in our case, we will use chi-squared and mutual information). Also, we need the **chi2** function and **mutual_info_classif** functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to specify that we will use SelectKBest function with the chi-squared statistic for feature selection (by setting parameter \"score_func\") and indicate the number of best features we want to find (by setting parameter \"k\"). We'll select 10,000 best features, so **k = 10,000**:<br><br>\n",
    "*Look up the documentation for SelectKBest if needed: https://bit.ly/2Re54Ch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_kbest = SelectKBest(score_func = chi2, k = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find the best features. To do that, we call the **fit_transform** method with our defined kbest function. The fit_transform method takes **2 inputs**: the vectorized representation of the data (NORM_corpus_train_bow) and the array of class labels contained in the object corpus_train.target. To see the chi-squared scores for the features use method **scores_**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.43001686, 2.48287671, 4.96575342, ..., 2.43001686, 4.86003373,\n",
       "       4.96575342])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORM_corpus_train_bow_chi2_BEST = chi2_kbest.fit_transform(NORM_corpus_train_bow,\n",
    "                                                           corpus_train.target)\n",
    "chi2_kbest.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, which features are best? The **get_support** method with parameter *indices* set to True will return the indecies of the best k features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,     5,     7, ..., 21052, 21059, 21060])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi2_best_features_ind = chi2_kbest.get_support(indices=True)\n",
    "chi2_best_features_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the names of the best features, according to the chi-squared statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = bow_vectorizer.get_feature_names()\n",
    "chi2_best_features_names = np.array(features_names)[chi2_best_features_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data vectorized using best features selected based on the chi-squared statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000usd</th>\n",
       "      <th>00pm</th>\n",
       "      <th>023b</th>\n",
       "      <th>0x</th>\n",
       "      <th>1024x768</th>\n",
       "      <th>10bps</th>\n",
       "      <th>10km</th>\n",
       "      <th>10m</th>\n",
       "      <th>110m</th>\n",
       "      <th>115m</th>\n",
       "      <th>...</th>\n",
       "      <th>zoroaster</th>\n",
       "      <th>zoroastrian</th>\n",
       "      <th>zoroastrianism</th>\n",
       "      <th>zoroastrians</th>\n",
       "      <th>zubin</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zullen</th>\n",
       "      <th>zurvanism</th>\n",
       "      <th>zwarte</th>\n",
       "      <th>zyxel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000usd  00pm  023b  0x  1024x768  10bps  10km  10m  110m  115m  ...  \\\n",
       "0       0     0     0   0         0      0     0    0     0     0  ...   \n",
       "1       0     0     0   0         0      0     0    0     0     0  ...   \n",
       "2       0     0     0   0         0      0     0    0     0     0  ...   \n",
       "3       0     0     0   0         0      0     0    0     0     0  ...   \n",
       "4       0     0     0   0         0      0     0    0     0     0  ...   \n",
       "\n",
       "   zoroaster  zoroastrian  zoroastrianism  zoroastrians  zubin  zuck  zullen  \\\n",
       "0          0            0               0             0      0     0       0   \n",
       "1          0            0               0             0      0     0       0   \n",
       "2          0            0               0             0      0     0       0   \n",
       "3          0            0               0             0      0     0       0   \n",
       "4          0            0               0             0      0     0       0   \n",
       "\n",
       "   zurvanism  zwarte  zyxel  \n",
       "0          0       0      0  \n",
       "1          0       0      0  \n",
       "2          0       0      0  \n",
       "3          0       0      0  \n",
       "4          0       0      0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert vectorized data to a dataframe and give columns their names\n",
    "NORM_arr = NORM_corpus_train_bow_chi2_BEST.todense()\n",
    "X_train_bow_chi2_BEST_table = pd.DataFrame(data = NORM_arr,\n",
    "                                           columns = chi2_best_features_names)\n",
    "X_train_bow_chi2_BEST_table.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **<font color=green>EXERCISE 3:</font>**\n",
    "**<font color=green>3.1. Select best features using the mutual information statistic. Follow the steps we used for the chi-squared statistic. The first line, showing how to specify the SelectKBest function using mutual information, is provided. You need to complete the script.</font>** <br><br>\n",
    "*Note: you can check out the documentation for mutual information function for categorical data here: https://bit.ly/2JGgeeT* <br><br>\n",
    "Your code (add more lines):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_kbest = SelectKBest(score_func = mutual_info_classif, k = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00060627, 0.0006138 , 0.0006138 , ..., 0.00060627, 0.00060627,\n",
       "       0.0012282 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORM_corpus_train_bow_MI_BEST = MI_kbest.fit_transform(NORM_corpus_train_bow,\n",
    "                                                       corpus_train.target)\n",
    "MI_kbest.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    7,    17,    35, ..., 21044, 21052, 21060])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_best_features_ind = MI_kbest.get_support(indices=True)\n",
    "MI_best_features_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = bow_vectorizer.get_feature_names()\n",
    "MI_best_features_names = np.array(features_names)[MI_best_features_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>023b</th>\n",
       "      <th>0x</th>\n",
       "      <th>1024x768</th>\n",
       "      <th>10km</th>\n",
       "      <th>10m</th>\n",
       "      <th>13h</th>\n",
       "      <th>15m</th>\n",
       "      <th>15rpm</th>\n",
       "      <th>17th</th>\n",
       "      <th>18084tm</th>\n",
       "      <th>...</th>\n",
       "      <th>zorastrian</th>\n",
       "      <th>zoro</th>\n",
       "      <th>zoroaster</th>\n",
       "      <th>zoroastrian</th>\n",
       "      <th>zoroastrianism</th>\n",
       "      <th>zoroastrians</th>\n",
       "      <th>zubin</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zurvanism</th>\n",
       "      <th>zyxel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   023b  0x  1024x768  10km  10m  13h  15m  15rpm  17th  18084tm  ...  \\\n",
       "0     0   0         0     0    0    0    0      0     0        0  ...   \n",
       "1     0   0         0     0    0    0    0      0     0        0  ...   \n",
       "2     0   0         0     0    0    0    0      0     0        0  ...   \n",
       "3     0   0         0     0    0    0    0      0     0        0  ...   \n",
       "4     0   0         0     0    0    0    0      0     0        0  ...   \n",
       "\n",
       "   zorastrian  zoro  zoroaster  zoroastrian  zoroastrianism  zoroastrians  \\\n",
       "0           0     0          0            0               0             0   \n",
       "1           0     0          0            0               0             0   \n",
       "2           0     0          0            0               0             0   \n",
       "3           0     0          0            0               0             0   \n",
       "4           0     0          0            0               0             0   \n",
       "\n",
       "   zubin  zuck  zurvanism  zyxel  \n",
       "0      0     0          0      0  \n",
       "1      0     0          0      0  \n",
       "2      0     0          0      0  \n",
       "3      0     0          0      0  \n",
       "4      0     0          0      0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORM_arr = NORM_corpus_train_bow_MI_BEST.todense()\n",
    "X_train_bow_MI_BEST_table = pd.DataFrame(data = NORM_arr,\n",
    "                                           columns = MI_best_features_names)\n",
    "X_train_bow_MI_BEST_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(X_train_bow_chi2_BEST_table.columns)- set(X_train_bow_MI_BEST_table.columns) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>3.2. Do mutual information approach and chi-squared statistic select the same best features?</font>** <br>\n",
    "\n",
    "**ANSWER:** The two select almost the same best features. The trow approaches differ by 1408 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for TEST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize and vectorize the **TEST** text corpus (corpus_test.data) using the Bag-of-Words method. <br><br>First, we normalize the testing corpus and call it NORM_corpus_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_corpus_test = normalize_corpus(corpus_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's vectorize the normalized test corpus, NORM_corpus_test. <br><br>\n",
    "**IMPORTANT! For transforming test data, you'll use features extracted from the training corpus [You do NOT want to create new feature based on your test data].** <br><br>Therefore: <br> 1) do **not** define a new vectorizer, use the one used on training data <br>2) use method **.transform** (not .fit_trandform) with your vectorizer to vectorize the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_corpus_test_bow = bow_vectorizer.transform(NORM_corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the best features selected by **chi-squared statistic**. Now we need to pick up from the above Bag-of-Words matrix exactly the same best features we selected for our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1353, 10000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORM_corpus_test_bow_chi2_BEST = chi2_kbest.transform(NORM_corpus_test_bow)\n",
    "NORM_corpus_test_bow_chi2_BEST.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "\n",
    "We'll train a text classification model that categorizes documents into 4 classes: religion, atheism, science and computer graphics. We will use a Naive Bayes Classifier and Support Vector Machines (SVM), the last one is optional.\n",
    "\n",
    "#### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the **MultinomialNB** packages available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Naive Bayes classifier by specifiying the hyperparameter alpha and call the classifier NB_tc:<br><br>\n",
    "*Note: you can set the hyperparameter alpha to an optimal value by trying different values > 0. With alpha = 0, you model will assign a probability of zero to a document in the test data if the document contains a feature not found in the training data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_tc = MultinomialNB(alpha=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model using the best features selected using the **chi-squared statistics**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_tc.fit(NORM_corpus_train_bow_chi2_BEST, corpus_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_nb_chi2_best = NB_tc.predict(NORM_corpus_test_bow_chi2_BEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the predictive power for the Naive Bayes classifier using chi-squared k=10,000 best features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                 231              7         25                  56\n",
      "comp.graphics                15            349         23                   2\n",
      "sci.space                    20             18        349                   7\n",
      "talk.religion.misc           77              5         19                 150 \n",
      "\n",
      "Accuracy rate:  0.7974870657797487 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_chi2_best = metrics.confusion_matrix(corpus_test.target,\n",
    "                                        predicted_nb_chi2_best)\n",
    "\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_chi2_best , \n",
    "                                           columns = corpus_train.target_names,\n",
    "                                           index = corpus_train.target_names),\"\\n\")\n",
    "print(\"Accuracy rate: \", metrics.accuracy_score(corpus_test.target, predicted_nb_chi2_best),\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green>EXERCISE 4:</font>**\n",
    "**<font color=green>4.1. Train the Naive Bayes classifier without doing feature selection, that is use all the features available in the normalized corpus. </font>** <br><br>\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_tc.fit(NORM_corpus_train_bow, corpus_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_nb = NB_tc.predict(NORM_corpus_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                 227              6         24                  62\n",
      "comp.graphics                12            350         23                   4\n",
      "sci.space                    19             18        348                   9\n",
      "talk.religion.misc           79              8         18                 146 \n",
      "\n",
      "Accuracy rate:  0.7915742793791575 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_mb = metrics.confusion_matrix(corpus_test.target, predicted_nb)\n",
    "\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_mb , \n",
    "                                           columns = corpus_train.target_names,\n",
    "                                           index = corpus_train.target_names),\"\\n\")\n",
    "\n",
    "print(\"Accuracy rate: \", metrics.accuracy_score(corpus_test.target,\n",
    "                                                predicted_nb),\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>What accuracy do you get? If a classifier did a mistake and misclassified a \"Computer Graphics\" post, to which class such a post was mistakenly assigned, typically? What about a post on the \"Atheism\" topic? </font>** <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWERS:** \n",
    "1. Accuracy: 0.7915 \n",
    "2. comp.graphics posts are misclassified as sci.space\n",
    "3. alt.atheism posts are misclassified as talk.religion.misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>4.2. (OPTIONAL) Train the Naive Bayes classifier feature selection based on mutual information (MI). What accuracy do you get?**<br><br>\n",
    "Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_tc.fit(NORM_corpus_train_bow_MI_BEST, corpus_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1353, 10000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORM_corpus_test_bow_MI = MI_kbest.transform(NORM_corpus_test_bow)\n",
    "NORM_corpus_test_bow_MI.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_nb_MI = NB_tc.predict(NORM_corpus_test_bow_MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                 237              7         24                  51\n",
      "comp.graphics                10            351         24                   4\n",
      "sci.space                    20             18        349                   7\n",
      "talk.religion.misc           84              7         19                 141 \n",
      "\n",
      "Accuracy rate:  0.7967479674796748 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_MI = metrics.confusion_matrix(corpus_test.target, predicted_nb_MI)\n",
    "\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_MI , \n",
    "                                           columns = corpus_train.target_names,\n",
    "                                           index = corpus_train.target_names),\"\\n\")\n",
    "\n",
    "print(\"Accuracy rate: \", metrics.accuracy_score(corpus_test.target,\n",
    "                                                predicted_nb_MI),\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** Accurary 0.7967"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green>EXERCISE 5 (OPTIONAL):</font>**\n",
    "**<font color=green>5.1. Vectorize the data using the TF-IDF approach, with and without feature selection, and train and test the Naive Bayes classifier. What are your results? </font>** <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier - TF-IDF Approach Without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(norm = 'l2',\n",
    "                                    smooth_idf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_corpus_train_tfidf = tfidf_vectorizer.fit_transform(NORM_corpus_train)\n",
    "NORM_corpus_test_tfidf = tfidf_vectorizer.transform(NORM_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_tc.fit(NORM_corpus_train_tfidf, corpus_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_nb_tfidf = NB_tc.predict(NORM_corpus_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier - TF-IDF Approach With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "NORM_corpus_train_tfidf_MI_BEST = MI_kbest.fit_transform(NORM_corpus_train_tfidf,\n",
    "                                                       corpus_train.target)\n",
    "NORM_corpus_train_tfidf_chi2_BEST = chi2_kbest.fit_transform(NORM_corpus_train_tfidf,\n",
    "                                                           corpus_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "NORM_corpus_test_tfidf_MI_BEST = MI_kbest.transform(NORM_corpus_test_tfidf)\n",
    "NORM_corpus_test_tfidf_chi2_BEST = chi2_kbest.transform(NORM_corpus_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_tc.fit(NORM_corpus_train_tfidf_chi2_BEST, corpus_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tfidf_chi2_BEST = NB_tc.predict(NORM_corpus_test_tfidf_chi2_BEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes Classifier - TF-IDF Approach WITHOUT Feature Selection\n",
      "\n",
      "Confusion matrix: \n",
      "                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                 224              7         39                  49\n",
      "comp.graphics                10            350         27                   2\n",
      "sci.space                    19             18        354                   3\n",
      "talk.religion.misc           84              8         20                 139 \n",
      "\n",
      "Accuracy rate:  0.7886178861788617 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_tfidf = metrics.confusion_matrix(corpus_test.target, predicted_nb_tfidf)\n",
    "\n",
    "print(\"\\nNaive Bayes Classifier - TF-IDF Approach WITHOUT Feature Selection\\n\")\n",
    "\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_tfidf , \n",
    "                                           columns = corpus_train.target_names,\n",
    "                                           index = corpus_train.target_names),\"\\n\")\n",
    "\n",
    "print(\"Accuracy rate: \", metrics.accuracy_score(corpus_test.target,\n",
    "                                                predicted_nb_tfidf),\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes Classifier - TF-IDF Approach WITH Feature Selection\n",
      "\n",
      "Confusion matrix: \n",
      "                     alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                 221              8         38                  52\n",
      "comp.graphics                11            351         26                   1\n",
      "sci.space                    19             16        357                   2\n",
      "talk.religion.misc           79              7         21                 144 \n",
      "\n",
      "Accuracy rate:  0.7930524759793053 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_tfidf_best = metrics.confusion_matrix(corpus_test.target, predicted_tfidf_chi2_BEST)\n",
    "\n",
    "print(\"\\nNaive Bayes Classifier - TF-IDF Approach WITH Feature Selection\\n\")\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_tfidf_best, \n",
    "                                           columns = corpus_train.target_names,\n",
    "                                           index = corpus_train.target_names),\"\\n\")\n",
    "\n",
    "print(\"Accuracy rate: \", metrics.accuracy_score(corpus_test.target,\n",
    "                                                predicted_tfidf_chi2_BEST),\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
